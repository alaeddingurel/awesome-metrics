# awesome-metrics
This repository contains mostly about the metrics which are used in the Natural Language Processing, Machine Learning and Deep Learning field.

# TODO
- we could list them for each topich because f1 for document classification and named entity recognition is different.

1. [Accuracy](./metrics/accuracy.md)
2. [Precision](./metrics/precision.md)
3. [Recall](./metrics/recall.md)
4. [F1-Score](./metrics/f1score.md)
5. [ROC-AUC](./metrics/rocauc.md)
6. [PR-AUC (Precision-Recall Area Under Curve)](./metrics/prauc.md)
7. [BLEU (Bilingual Evaluation Understudy)](./metrics/bleu.md)
8. [METEOR (Metric for Evaluation of Translation with Explicit ORdering)](./metrics/bleu.md)
9. TER (Translation Edit Rate)
10. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
11. CIDEr (Consensus-based Image Description Evaluation)
12. [WER (Word Error Rate)](./metrics/wer.md)
13. PER (Phone Error Rate)
14. [SER (Sentence Error Rate)](./metrics/ser.md)
15. NIST (NIST Automated Evaluation of Machine Translation)
16. [GLEU (Google-BLEU)](./metrics/precision.md)
17. [CharacTER (Character Error Rate)](./metrics/precision.md)
18. SARI (System-level Automatic Reviewer for Text)
19. [N-gram overlap (e.g., bigram, trigram)](./metrics/precision.md)
20. [Jaccard similarity](./metrics/jaccard.md)
21. [Cosine similarity](./metrics/cosine.md)
22. [Euclidean distance](./metrics/euclidian.md)
23. [Manhattan distance](./metrics/manhattan.md)
24. Mahalanobis distance
25. [Perplexity](./metrics/perplexity.md)
26. [Entropy](./metrics/entropy.md)Entropy
27. [Kullback-Leibler divergence (KL divergence)](./metrics/entropy.md)
28. [Jensen-Shannon divergence](./metrics/jensen_shannon.md)Jensen-Shannon divergence
32. [Edit distance (Levenshtein distance)](./metrics/edit_distance.md)
33. [Edit similarity (Jaro-Winkler similarity)](./metrics/edit_similarity.md)
34. Language modeling evaluation (e.g., PPL, BPC)
37. Dependency parsing accuracy
38. Chunking F1-Score
39. Named Entity Disambiguation (NED) accuracy
44. Information retrieval metrics (e.g., Precision@k, Recall@k)
45. Text summarization metrics (e.g., ROUGE for summaries)
46. Dialogue evaluation metrics (e.g., Perplexity, BLEU for chatbots)
47. Discourse coherence metrics
48. Readability scores (e.g., Flesch-Kincaid, Gunning Fog Index)



[Link text Here](https://link-url-here.org)